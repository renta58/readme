# Analisis Clustering Produk Farmasi di Sumatera Menggunakan Ekosistem Hadoop
## Deskripsi Produk 
Proyek ini bertujuan untuk melakukan analisis clustering terhadap data produk farmasi di Wilayah Sumatera menggunakan Algoritma **K-Means** dalam **Ekosistem Hadoop** .
## Tujuan
Proyek ini bertujuan untuk membantu perusahan farmasi dalam mengelompokkan produk berdasarkan kategori wilayah, kateogri obat , dan informasi lainnya yang relevan sehingga pengelolaan ketersediaan obat yang lebih optimal dan tepa sasaran
## Dataset
Data yang digunakan merupakan data transaksi farmasi tahun **2020-2023** yang mencakup informasi produk, pelanggan, kategori, diskon, rating dan lokasi cabang

## Arsitektur Sistem 
Sistem dibangun berdasaran **Medalion Architectur**, yang membagi prpses pipeline menjaid tiga lapisan:
- **Bronze Layer**: Data mentah dari berbagai sumber.
- **Silver Layer**: Data hasil pembersihan, transformasi, dan integrasi.
- **Gold Layer**: Data siap pakai untuk analisis dan visualisasi.
## Infrastruktur teknologi
Proyek ini berjalan di lingkungan **Docker** yang berisi komponen-komponen sebagai berikut:

- **HDFS** â€“ Penyimpanan data terdistribusi
- **Apache Spark** â€“ Pemrosesan batch ETL dan analitik
- **Apache Hive** â€“ SQL query engine untuk eksplorasi data
- **Apache Superset** â€“ Visualisasi data interaktif
- **Hive Metastore** â€“ Penyimpanan metadata

Konfigurasi cluster lokal:
- 1 NameNode
- 2 DataNode
- 1 Spark Master
- 2 Spark Worker
  ## Proses Analitik
  Analisis clustering dilakukan dengan **K-Means Clustering** dari pustaka **Apache Spark MLlib**:

1. Membaca data dari Gold Layer (format Parquet)
2. Preprocessing: normalisasi, imputasi missing value
3. Split data ke training dan testing
4. Latih model K-Means
5. Evaluasi performa model
6. Simpan dan deploy model untuk penggunaan lebih lanjut

   ## Implementasi Pipeline
   Pipeline batch dibangun untuk memproses data secara bertahap dari ingest hingga analitik. Proses mencakup:

1. Ingest data ke HDFS
2. Transformasi dan pembersihan dengan Spark
3. Simpan hasil ke Hive
4. Analisis clustering dengan Spark MLlib
5. Visualisasi hasil

## pengujian sistem
Pengujian dilakukan secara menyeluruh:

- **Unit Testing** â€“ Validasi fungsi-fungsi individual (ETL, ingest, transformasi)
- **Integration Testing** â€“ Alur data dari HDFS ke Hive dan Superset
- **Data Quality Testing** â€“ Validasi duplikasi, missing value, dan schema
- **Performance Testing** â€“ Waktu eksekusi pipeline batch
- **End-to-End Testing** â€“ Simulasi jalur penuh dari data mentah ke visualisasi

## ðŸ“‚ Struktur Folder

```bash
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/               # Data mentah
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ hadoop/            # Konfigurasi Docker Hadoop
â”œâ”€â”€ etl/
â”‚   â””â”€â”€ spark_jobs.py      # Script ETL dan preprocessing
â”œâ”€â”€ model/
â”‚   â””â”€â”€ kmeans_model.pkl   # Model clustering
â”œâ”€â”€ report/
â”‚   â””â”€â”€ visualisasi.png    # Hasil clustering
â”œâ”€â”€ README.md

   
  
